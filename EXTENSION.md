# SimpleGPT 扩展指南

恭喜你已经跑通了最基础的 GPT 模型！如果你想进一步深入，以下是一些扩展和改进的方向。

## 1. 改进 Tokenizer (分词器)

目前的模型是**字符级 (Character-level)** 的。这意味着它把 'a', 'b', 'c' 当作基本单位。
*   **缺点**: 序列很长（一句话包含很多字符），且模型需要花费大量精力学习单词拼写。
*   **改进**: 使用 **Sub-word (子词)** 分词，如 **BPE (Byte Pair Encoding)**。
*   **工具**: 尝试使用 OpenAI 的 `tiktoken` 库或 HuggingFace 的 `tokenizers` 库。
    *   这将大大缩短序列长度，并提高模型的语义理解能力。

## 2. 扩大模型规模

在 `train.py` 中，你可以尝试增加以下参数（取决于你的显存大小）：
*   `n_layer`: 增加层数（例如从 2 到 6）。
*   `n_head`: 增加注意力头数（例如从 4 到 8）。
*   `n_embd`: 增加嵌入维度（例如从 128 到 384）。
*   `block_size`: 增加上下文长度（例如从 64 到 256）。

*注意：扩大模型后，需要相应地降低 `learning_rate` 并增加 `dropout` 以防止过拟合。*

## 3. 使用更好的数据集

莎士比亚数据集很有趣，但它是古英语且风格单一。
*   尝试下载维基百科文本、小说集或代码库作为训练数据。
*   数据越多，模型越“聪明”。

## 4. 添加学习率调度器 (Learning Rate Scheduler)

目前的 `learning_rate` 是固定的。
*   **改进**: 实现 Warmup（预热）和 Cosine Decay（余弦衰减）。
*   在训练初期线性增加学习率，然后在剩余训练过程中按余弦曲线降低学习率。这通常能带来更好的收敛效果。

## 5. 实现 Top-k / Top-p 采样

在 `model.py` 的 `generate` 函数中，目前使用的是直接从多项式分布中采样。
*   **改进**: 实现 `Nucleus Sampling (Top-p)` 或 `Top-k Sampling`。
*   这可以过滤掉概率非常低的尾部 token，让生成的文本更连贯、质量更高。

## 6. 加载预训练权重

这个项目是从头训练的 (Pre-training)。
*   **进阶**: 尝试使用 HuggingFace 的 `transformers` 库加载 GPT-2 的预训练权重，然后用你的数据进行**微调 (Fine-tuning)**。这将让你瞬间拥有一个强大的模型。

## 7. 交互式 Web 界面

*   使用 `Streamlit` 或 `Gradio` 编写一个简单的网页前端，替代命令行的 `generate.py`，让交互更友好。

---
*祝你在大模型探索之路上玩得开心！*
